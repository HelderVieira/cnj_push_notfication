{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf20e0d0-b661-4a30-958e-f082c27e21f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bibliotecas importadas.\n",
      "Configurações da API carregadas.\n",
      "Funções utilitárias definidas.\n"
     ]
    }
   ],
   "source": [
    "# Célula 1: Importações, Configurações e Funções Utilitárias\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import pymongo\n",
    "from datetime import datetime, timezone, timedelta # Adicionado timedelta\n",
    "import hashlib\n",
    "\n",
    "print(\"Bibliotecas importadas.\")\n",
    "\n",
    "# --- Configurações Globais ---\n",
    "API_BASE_URL = \"https://api-publica.datajud.cnj.jus.br/api_publica_{tribunal}/_search\"\n",
    "TAMANHO_PAGINA_API = 10000\n",
    "MAX_RETRIES_CONEXAO_API = 5\n",
    "TIMEOUT_REQUEST_API = 240\n",
    "\n",
    "ESTADO_BASE_PATH = r\"C:\\IFPB\\bd\\projeto\\estado_mongo\"\n",
    "if not os.path.exists(ESTADO_BASE_PATH):\n",
    "    os.makedirs(ESTADO_BASE_PATH)\n",
    "    print(f\"Diretório de estado '{ESTADO_BASE_PATH}' criado.\")\n",
    "\n",
    "SUA_API_KEY = \"cDZHYzlZa0JadVREZDJCendQbXY6SkJlTzNjLV9TRENyQk1RdnFKZGRQdw==\" # <--- SUBSTITUA\n",
    "\n",
    "HEADERS_API = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"ApiKey {SUA_API_KEY}\"\n",
    "}\n",
    "\n",
    "if SUA_API_KEY == \"COLOQUE_SUA_API_KEY_AQUI\":\n",
    "    print(\"!!! ATENÇÃO: API KEY NÃO CONFIGURADA. INTERROMPENDO. !!!\")\n",
    "    raise ValueError(\"API Key não configurada.\")\n",
    "\n",
    "print(\"Configurações da API carregadas.\")\n",
    "\n",
    "# --- FUNÇÕES UTILITÁRIAS GLOBAIS ---\n",
    "\n",
    "def consultar_api_datajud_mongo(tribunal, query_payload):\n",
    "    \"\"\"Faz uma requisição à API do DataJud com retries, tratando 429.\"\"\"\n",
    "    url = API_BASE_URL.format(tribunal=tribunal.lower())\n",
    "    response_data = None\n",
    "    last_exception_details = \"\"\n",
    "    for tentativa in range(MAX_RETRIES_CONEXAO_API + 3): # Aumentar um pouco para 429\n",
    "        try:\n",
    "            print(f\"  API Call: Tribunal {tribunal}, Tentativa {tentativa + 1}...\")\n",
    "            response = requests.post(url, json=query_payload, headers=HEADERS_API, timeout=TIMEOUT_REQUEST_API)\n",
    "            if response.status_code == 429:\n",
    "                retry_after_str = response.headers.get(\"Retry-After\")\n",
    "                espera = 30 + (2 ** tentativa) * 5 \n",
    "                if retry_after_str:\n",
    "                    try:\n",
    "                        espera_header = int(retry_after_str)\n",
    "                        espera = max(espera, espera_header) # Usa o maior tempo de espera\n",
    "                        print(f\"    API Rate Limit (429). Cabeçalho Retry-After: {espera_header}s. Aguardando {espera}s.\")\n",
    "                    except ValueError:\n",
    "                        print(f\"    API Rate Limit (429). Retry-After inválido ('{retry_after_str}'). Usando espera de {espera}s.\")\n",
    "                else:\n",
    "                    print(f\"    API Rate Limit (429) sem cabeçalho Retry-After. Usando espera de {espera}s.\")\n",
    "                if tentativa < MAX_RETRIES_CONEXAO_API + 2:\n",
    "                    time.sleep(espera)\n",
    "                    continue\n",
    "                else:\n",
    "                    last_exception_details = f\"Status 429: Too Many Requests. Máximo de retries atingido.\"\n",
    "                    break\n",
    "            response.raise_for_status()\n",
    "            response_data = response.json()\n",
    "            return response_data\n",
    "        except (requests.exceptions.ConnectionError, requests.exceptions.Timeout, requests.exceptions.ChunkedEncodingError) as e_net:\n",
    "            last_exception_details = str(e_net)\n",
    "            print(f\"    Erro de rede/timeout na tentativa {tentativa + 1}: {e_net}\")\n",
    "            if tentativa < MAX_RETRIES_CONEXAO_API + 2: time.sleep((2 ** tentativa) * 2)\n",
    "        except requests.exceptions.RequestException as e_req:\n",
    "            last_exception_details = f\"Status {response.status_code if response else 'N/A'}: {response.text[:500] if response else str(e_req)}\"\n",
    "            print(f\"    Erro HTTP não recuperável: {e_req}. Detalhes: {last_exception_details}\")\n",
    "            break \n",
    "        except json.JSONDecodeError as e_json:\n",
    "            last_exception_details = f\"JSONDecodeError: {str(e_json)}. Response: {response.text[:500] if response else 'N/A'}\"\n",
    "            print(f\"    Erro ao decodificar JSON: {e_json}. Response: {response.text[:500] if response else 'N/A'}\")\n",
    "            break\n",
    "        except Exception as e_geral:\n",
    "            last_exception_details = f\"Erro geral: {type(e_geral).__name__} - {str(e_geral)}\"\n",
    "            print(f\"    Erro geral inesperado na API call: {last_exception_details}\")\n",
    "            break\n",
    "    if response_data is None:\n",
    "        print(f\"    Falha final ao consultar API para {tribunal}. Último erro: {last_exception_details}\")\n",
    "    return response_data\n",
    "\n",
    "def string_para_datetime_utc(date_string):\n",
    "    if not date_string: return None\n",
    "    try:\n",
    "        # Lidar com formatos YYYY-MM-DDTHH:mm:ss.SSSSSSSSSZ e YYYY-MM-DDTHH:mm:ssZ\n",
    "        # e também YYYY-MM-DDTHH:mm:ss.SSS+HH:MM\n",
    "        original_date_string = date_string # Para log em caso de falha\n",
    "        \n",
    "        # Normalizar 'Z' para +00:00\n",
    "        if date_string.endswith('Z'):\n",
    "            date_string = date_string[:-1] + '+00:00'\n",
    "        \n",
    "        # Truncar nanossegundos para microssegundos (6 dígitos) se houver fração de segundo\n",
    "        if '.' in date_string:\n",
    "            parts = date_string.split('.')\n",
    "            datetime_part = parts[0]\n",
    "            fraction_and_tz_part = parts[1]\n",
    "\n",
    "            # Encontrar o início do fuso horário, se houver\n",
    "            tz_index_plus = fraction_and_tz_part.rfind('+')\n",
    "            tz_index_minus = fraction_and_tz_part.rfind('-')\n",
    "            \n",
    "            tz_char_index = -1\n",
    "            if tz_index_plus != -1 and tz_index_minus != -1:\n",
    "                tz_char_index = min(tz_index_plus, tz_index_minus)\n",
    "            elif tz_index_plus != -1:\n",
    "                tz_char_index = tz_index_plus\n",
    "            elif tz_index_minus != -1:\n",
    "                tz_char_index = tz_index_minus\n",
    "\n",
    "            if tz_char_index != -1:\n",
    "                microseconds_part = fraction_and_tz_part[:tz_char_index][:6]\n",
    "                tz_part = fraction_and_tz_part[tz_char_index:]\n",
    "            else: # Sem fuso horário explícito após a fração\n",
    "                microseconds_part = fraction_and_tz_part[:6]\n",
    "                tz_part = '+00:00' # Assume UTC se não especificado e Z foi removido\n",
    "            \n",
    "            date_string = f\"{datetime_part}.{microseconds_part}{tz_part}\"\n",
    "        elif not ('+' in date_string or date_string.count('-') > 2): # Sem fração e sem fuso explícito\n",
    "             date_string += '+00:00'\n",
    "\n",
    "\n",
    "        dt_obj = datetime.fromisoformat(date_string)\n",
    "        # Garantir que seja UTC\n",
    "        return dt_obj.astimezone(timezone.utc)\n",
    "    except Exception as e:\n",
    "        # print(f\"    Aviso: Não foi possível converter a string de data '{original_date_string}' para datetime. Erro: {e}. Retornando None.\")\n",
    "        return None\n",
    "\n",
    "def converter_datas_no_documento(doc):\n",
    "    if isinstance(doc, dict):\n",
    "        return {k: converter_datas_no_documento(v) for k, v in doc.items()}\n",
    "    elif isinstance(doc, list):\n",
    "        return [converter_datas_no_documento(elem) for elem in doc]\n",
    "    elif isinstance(doc, str):\n",
    "        # Tentar converter apenas campos que parecem ser datas para evitar conversões erradas\n",
    "        # Heurística: tem 'T', tem 'Z' ou fuso, e tem pelo menos 19 chars\n",
    "        if len(doc) >= 19 and 'T' in doc and (doc.endswith('Z') or '+' in doc or doc.count('-') > 2):\n",
    "            converted_date = string_para_datetime_utc(doc)\n",
    "            return converted_date if converted_date is not None else doc # Mantém string se falhar\n",
    "    return doc\n",
    "\n",
    "# Funções de estado (agora só para search_after da coleta inicial)\n",
    "def salvar_estado_coleta_inicial_mongo(tribunal, search_after_param):\n",
    "    estado = {\"search_after_coleta_inicial\": search_after_param}\n",
    "    path_estado = os.path.join(ESTADO_BASE_PATH, f\"coleta_inicial_mongo_{tribunal}_estado.json\")\n",
    "    try:\n",
    "        with open(path_estado, 'w') as f_estado: json.dump(estado, f_estado)\n",
    "    except Exception as e: print(f\"  ERRO ao salvar estado de coleta inicial para {tribunal}: {e}\")\n",
    "\n",
    "def carregar_estado_coleta_inicial_mongo(tribunal):\n",
    "    path_estado = os.path.join(ESTADO_BASE_PATH, f\"coleta_inicial_mongo_{tribunal}_estado.json\")\n",
    "    if os.path.exists(path_estado):\n",
    "        try:\n",
    "            with open(path_estado, 'r') as f_estado:\n",
    "                estado = json.load(f_estado)\n",
    "                print(f\"  Estado de COLETA INICIAL MongoDB para {tribunal} carregado: SA={estado.get('search_after_coleta_inicial')}\")\n",
    "                return estado.get(\"search_after_coleta_inicial\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Erro ao carregar estado de COLETA INICIAL para {tribunal}: {e}. Iniciando do começo.\")\n",
    "    return None\n",
    "\n",
    "print(\"Funções utilitárias definidas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d595eae-f65d-448c-b375-35e2a8991322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f06b1b02-ff5e-4ff8-bc72-e48f634849bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Verificando Quantidade de Processos (Estimativa da API) ---\n",
      "\n",
      "Consultando tribunal: TJPB\n",
      "  API Call: Tribunal tjpb, Tentativa 1...\n",
      "  Estimativa de processos para TJPB: 2694792 (relação: eq)\n"
     ]
    }
   ],
   "source": [
    "# Célula 2: Consultar Quantidade de Processos na API\n",
    "\n",
    "# tribunais_para_verificar_qtd = [\"tjpb\", \"trf5\", \"stj\"] # Exemplo de lista\n",
    "tribunais_para_verificar_qtd = [\"tjpb\"] # Para focar em um para a coleta principal\n",
    "\n",
    "print(\"\\n--- Verificando Quantidade de Processos (Estimativa da API) ---\")\n",
    "for tribunal_sigla_qtd in tribunais_para_verificar_qtd:\n",
    "    print(f\"\\nConsultando tribunal: {tribunal_sigla_qtd.upper()}\")\n",
    "    query_contagem_api = {\n",
    "        \"query\": {\"match_all\": {}},\n",
    "        \"size\": 0,\n",
    "        \"track_total_hits\": True\n",
    "    }\n",
    "    \n",
    "    data_api_qtd = consultar_api_datajud_mongo(tribunal_sigla_qtd, query_contagem_api)\n",
    "    \n",
    "    if data_api_qtd and data_api_qtd.get('hits') and isinstance(data_api_qtd['hits'].get('total'), dict):\n",
    "        total_value_api = data_api_qtd['hits']['total'].get('value', 'N/A')\n",
    "        total_relation_api = data_api_qtd['hits']['total'].get('relation', 'N/A')\n",
    "        print(f\"  Estimativa de processos para {tribunal_sigla_qtd.upper()}: {total_value_api} (relação: {total_relation_api})\")\n",
    "    else:\n",
    "        print(f\"  Não foi possível obter a contagem para {tribunal_sigla_qtd.upper()}. Resposta: {str(data_api_qtd)[:200] if data_api_qtd else 'Nenhuma resposta'}\")\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23e8b4cc-700b-4576-a911-7c144ec9358f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Conectando ao MongoDB em mongodb://localhost:27017/ ---\n",
      "Conexão com MongoDB bem-sucedida!\n",
      "Usando/Criando banco de dados: 'base_cnj'\n",
      "Usando/Criando coleção de processos: 'processos_tjpb'\n",
      "  Índice padrão em '_id' para 'processos_tjpb' é automaticamente único.\n",
      "  Índice em '@timestamp' para 'processos_tjpb' garantido.\n",
      "Usando/Criando coleção de movimentações: 'movimentacoes_tjpb'\n",
      "  Índice padrão em '_id' para 'movimentacoes_tjpb' é automaticamente único.\n",
      "  Outros índices para 'movimentacoes_tjpb' criados/verificados.\n"
     ]
    }
   ],
   "source": [
    "# Célula 3: Conexão com MongoDB e Definição de Coleções (CORRIGIDA NOVAMENTE)\n",
    "\n",
    "MONGO_URI_CONEXAO = \"mongodb://localhost:27017/\"\n",
    "NOME_BANCO_DADOS_MONGO = \"base_cnj\"\n",
    "TRIBUNAL_ALVO_MONGO_COLETA = \"tjpb\" # Mude conforme necessário\n",
    "COLECAO_PROCESSOS_MONGO_NOME = f\"processos_{TRIBUNAL_ALVO_MONGO_COLETA.lower()}\"\n",
    "COLECAO_MOVIMENTACOES_MONGO_NOME = f\"movimentacoes_{TRIBUNAL_ALVO_MONGO_COLETA.lower()}\"\n",
    "\n",
    "mongo_client = None\n",
    "db_mongo = None\n",
    "colecao_processos_mongo = None\n",
    "colecao_movimentacoes_mongo = None\n",
    "\n",
    "try:\n",
    "    print(f\"\\n--- Conectando ao MongoDB em {MONGO_URI_CONEXAO} ---\")\n",
    "    mongo_client = pymongo.MongoClient(MONGO_URI_CONEXAO, serverSelectionTimeoutMS=5000)\n",
    "    mongo_client.admin.command('ping')\n",
    "    print(\"Conexão com MongoDB bem-sucedida!\")\n",
    "\n",
    "    db_mongo = mongo_client[NOME_BANCO_DADOS_MONGO]\n",
    "    print(f\"Usando/Criando banco de dados: '{NOME_BANCO_DADOS_MONGO}'\")\n",
    "\n",
    "    colecao_processos_mongo = db_mongo[COLECAO_PROCESSOS_MONGO_NOME]\n",
    "    print(f\"Usando/Criando coleção de processos: '{COLECAO_PROCESSOS_MONGO_NOME}'\")\n",
    "    print(f\"  Índice padrão em '_id' para '{COLECAO_PROCESSOS_MONGO_NOME}' é automaticamente único.\")\n",
    "    try:\n",
    "        # Índice para buscar o @timestamp mais recente da API\n",
    "        colecao_processos_mongo.create_index([(\"@timestamp\", pymongo.DESCENDING)], \n",
    "                                             name=\"idx_processo_api_timestamp\", \n",
    "                                             background=True)\n",
    "        print(f\"  Índice em '@timestamp' para '{COLECAO_PROCESSOS_MONGO_NOME}' garantido.\")\n",
    "    except Exception as e_idx_ts:\n",
    "        print(f\"  Aviso ao criar/verificar índice em '@timestamp' para processos: {e_idx_ts}\")\n",
    "\n",
    "\n",
    "    colecao_movimentacoes_mongo = db_mongo[COLECAO_MOVIMENTACOES_MONGO_NOME]\n",
    "    print(f\"Usando/Criando coleção de movimentações: '{COLECAO_MOVIMENTACOES_MONGO_NOME}'\")\n",
    "    # O MongoDB cria automaticamente um índice único em _id para a coleção de movimentações também.\n",
    "    # A linha abaixo foi REMOVIDA:\n",
    "    # colecao_movimentacoes_mongo.create_index([(\"_id\", pymongo.ASCENDING)], name=\"idx_mov_id_unico\", background=True, unique=True) \n",
    "    print(f\"  Índice padrão em '_id' para '{COLECAO_MOVIMENTACOES_MONGO_NOME}' é automaticamente único.\")\n",
    "    \n",
    "    # Outros índices úteis para a coleção de movimentações\n",
    "    try:\n",
    "        colecao_movimentacoes_mongo.create_index([(\"processo_id\", pymongo.ASCENDING)], \n",
    "                                                 name=\"idx_mov_processo_id\", \n",
    "                                                 background=True)\n",
    "        colecao_movimentacoes_mongo.create_index([(\"processo_id\", pymongo.ASCENDING), (\"dataHora\", pymongo.ASCENDING)], \n",
    "                                                 name=\"idx_mov_proc_datahora\", \n",
    "                                                 background=True)\n",
    "        print(f\"  Outros índices para '{COLECAO_MOVIMENTACOES_MONGO_NOME}' criados/verificados.\")\n",
    "    except Exception as e_idx_mov:\n",
    "        print(f\"  Aviso ao criar/verificar outros índices para movimentações: {e_idx_mov}\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERRO inesperado durante a configuração do MongoDB: {type(e).__name__} - {e}\")\n",
    "    if 'mongo_client' in locals() and mongo_client is not None: # Tentar fechar se foi aberto\n",
    "        mongo_client.close()\n",
    "    mongo_client = None # Garantir que está None se a conexão falhar\n",
    "\n",
    "if mongo_client is None:\n",
    "    print(\"!!! ATENÇÃO: Não foi possível conectar ou configurar o MongoDB corretamente. As próximas células de inserção não funcionarão. !!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91d0668f-a3b7-4d8b-b0e5-ed4688870f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!! ATENÇÃO: MongoDB não está conectado. Pulando Célula 4. !!!\n"
     ]
    }
   ],
   "source": [
    "# Célula 3: Coleta Inicial e Retomada para MongoDB (Ajustada)\n",
    "\n",
    "BATCH_SIZE_MONGO_INSERT_C4 = 100000 # Ou o valor que você preferir\n",
    "\n",
    "if 'mongo_client' not in locals() or mongo_client is None:\n",
    "    print(\"!!! ATENÇÃO: MongoDB não está conectado. Pulando Célula 4. !!!\")\n",
    "else:\n",
    "    print(f\"\\n--- Iniciando Coleta INICIAL/RETOMADA para MongoDB do Tribunal: {TRIBUNAL_ALVO_MONGO_COLETA.upper()} (Célula 4) ---\")\n",
    "    \n",
    "    search_after_coleta_inicial_c4 = carregar_estado_coleta_inicial_mongo(TRIBUNAL_ALVO_MONGO_COLETA)\n",
    "    \n",
    "    total_api_lidos_nesta_sessao_c4 = 0\n",
    "    primeira_pagina_coleta_c4 = not bool(search_after_coleta_inicial_c4)\n",
    "    \n",
    "    processos_batch_mongo_c4 = []\n",
    "    movimentacoes_batch_mongo_c4 = []\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            query_api_c4 = {\n",
    "                \"size\": TAMANHO_PAGINA_API,\n",
    "                \"query\": {\"match_all\": {}},\n",
    "                \"sort\": [{\"@timestamp\": \"asc\"}] \n",
    "            }\n",
    "            if search_after_coleta_inicial_c4:\n",
    "                query_api_c4[\"search_after\"] = search_after_coleta_inicial_c4\n",
    "            \n",
    "            response_data_c4 = consultar_api_datajud_mongo(TRIBUNAL_ALVO_MONGO_COLETA, query_api_c4)\n",
    "\n",
    "            if response_data_c4 is None:\n",
    "                print(f\"  Falha ao obter dados da API. Encerrando coleta (C4).\")\n",
    "                break\n",
    "\n",
    "            hits_api_c4 = response_data_c4.get('hits', {}).get('hits', [])\n",
    "\n",
    "            if primeira_pagina_coleta_c4:\n",
    "                total_estimado_api_c4 = response_data_c4.get('hits', {}).get('total', {}).get('value', 0)\n",
    "                print(f\"  Total de processos estimados (API) para coleta inicial: {total_estimado_api_c4}\")\n",
    "                primeira_pagina_coleta_c4 = False\n",
    "                if total_estimado_api_c4 == 0: break\n",
    "            \n",
    "            if not hits_api_c4:\n",
    "                print(f\"  Nenhum processo retornado nesta página. Fim da coleta (C4).\")\n",
    "                break\n",
    "\n",
    "            # Processar os hits (lógica de transformação e append aos batches como antes)\n",
    "            for hit_api in hits_api_c4:\n",
    "                source_data_api = hit_api.get(\"_source\", {})\n",
    "                id_cnj_original_api = hit_api.get(\"_id\")\n",
    "                if not id_cnj_original_api: continue\n",
    "                id_processo_mongo = id_cnj_original_api\n",
    "                \n",
    "                processo_doc = {\"_id\": id_processo_mongo}\n",
    "                processo_doc.update(source_data_api)\n",
    "                movs_do_processo_api = processo_doc.pop('movimentos', [])\n",
    "                processo_doc = converter_datas_no_documento(processo_doc)\n",
    "                processo_doc[\"_timestampColetaLocal\"] = datetime.now(timezone.utc)\n",
    "                processos_batch_mongo_c4.append(processo_doc)\n",
    "\n",
    "                if movs_do_processo_api and isinstance(movs_do_processo_api, list):\n",
    "                    for mov_idx, mov_api_data in enumerate(movs_do_processo_api):\n",
    "                        if isinstance(mov_api_data, dict):\n",
    "                            mov_doc = mov_api_data.copy()\n",
    "                            mov_doc[\"processo_id\"] = id_processo_mongo\n",
    "                            mov_hash_parts = [\n",
    "                                str(id_processo_mongo), str(mov_api_data.get(\"codigo\")),\n",
    "                                str(mov_api_data.get(\"dataHora\")), \n",
    "                                json.dumps(mov_api_data.get(\"complementosTabelados\"), sort_keys=True),\n",
    "                                str(mov_idx) \n",
    "                            ]\n",
    "                            mov_doc[\"_id\"] = hashlib.sha1(\"\".join(filter(None, mov_hash_parts)).encode('utf-8')).hexdigest()\n",
    "                            mov_doc = converter_datas_no_documento(mov_doc)\n",
    "                            mov_doc[\"_timestampColetaLocal\"] = datetime.now(timezone.utc)\n",
    "                            movimentacoes_batch_mongo_c4.append(mov_doc)\n",
    "            \n",
    "            total_api_lidos_nesta_sessao_c4 += len(hits_api_c4)\n",
    "            # A contagem global agora é implícita pelo que está no banco\n",
    "            print(f\"    Lidos {len(hits_api_c4)} da API. Total sessão (C4): {total_api_lidos_nesta_sessao_c4}.\")\n",
    "\n",
    "            if len(processos_batch_mongo_c4) >= BATCH_SIZE_MONGO_INSERT_C4:\n",
    "                print(f\"    Inserindo/Atualizando {len(processos_batch_mongo_c4)} processos no MongoDB...\")\n",
    "                bulk_ops_proc = [pymongo.ReplaceOne({\"_id\": doc[\"_id\"]}, doc, upsert=True) for doc in processos_batch_mongo_c4]\n",
    "                if bulk_ops_proc: \n",
    "                    try: colecao_processos_mongo.bulk_write(bulk_ops_proc, ordered=False)\n",
    "                    except pymongo.errors.BulkWriteError as bwe_proc: print(f\"      Erro de BulkWrite (processos): {bwe_proc.details}\")\n",
    "                processos_batch_mongo_c4 = []\n",
    "                \n",
    "                print(f\"    Inserindo/Atualizando {len(movimentacoes_batch_mongo_c4)} movimentações...\")\n",
    "                bulk_ops_mov = [pymongo.ReplaceOne({\"_id\": mov[\"_id\"]}, mov, upsert=True) for mov in movimentacoes_batch_mongo_c4]\n",
    "                if bulk_ops_mov:\n",
    "                    try: colecao_movimentacoes_mongo.bulk_write(bulk_ops_mov, ordered=False)\n",
    "                    except pymongo.errors.BulkWriteError as bwe_mov: print(f\"      Erro de BulkWrite (movs): {bwe_mov.details}\")\n",
    "                movimentacoes_batch_mongo_c4 = []\n",
    "                print(f\"    Lotes inseridos/atualizados no MongoDB.\")\n",
    "\n",
    "            search_after_coleta_inicial_c4 = hits_api_c4[-1]['sort']\n",
    "            salvar_estado_coleta_inicial_mongo(TRIBUNAL_ALVO_MONGO_COLETA, search_after_coleta_inicial_c4)\n",
    "            time.sleep(0.1)\n",
    "\n",
    "    except Exception as e_fatal_mongo_coleta:\n",
    "        print(f\"\\nErro fatal durante a coleta inicial (C4): {type(e_fatal_mongo_coleta).__name__} - {e_fatal_mongo_coleta}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    except KeyboardInterrupt:\n",
    "        print(f\"\\nColeta inicial (C4) interrompida.\")\n",
    "    finally:\n",
    "        # Inserir batches restantes\n",
    "        if processos_batch_mongo_c4:\n",
    "            print(f\"    Inserindo/Atualizando {len(processos_batch_mongo_c4)} processos restantes (C4)...\")\n",
    "            # ... (lógica de bulk_write como acima)\n",
    "            bulk_ops_proc_final = [pymongo.ReplaceOne({\"_id\": doc[\"_id\"]}, doc, upsert=True) for doc in processos_batch_mongo_c4]\n",
    "            if bulk_ops_proc_final: \n",
    "                try: colecao_processos_mongo.bulk_write(bulk_ops_proc_final, ordered=False)\n",
    "                except pymongo.errors.BulkWriteError as bwe_p_f: print(f\"      Erro BW (final proc): {bwe_p_f.details}\")\n",
    "        if movimentacoes_batch_mongo_c4:\n",
    "            print(f\"    Inserindo/Atualizando {len(movimentacoes_batch_mongo_c4)} movimentações restantes (C4)...\")\n",
    "            # ... (lógica de bulk_write como acima)\n",
    "            bulk_ops_mov_final = [pymongo.ReplaceOne({\"_id\": mov[\"_id\"]}, mov, upsert=True) for mov in movimentacoes_batch_mongo_c4]\n",
    "            if bulk_ops_mov_final:\n",
    "                try: colecao_movimentacoes_mongo.bulk_write(bulk_ops_mov_final, ordered=False)\n",
    "                except pymongo.errors.BulkWriteError as bwe_m_f: print(f\"      Erro BW (final mov): {bwe_m_f.details}\")\n",
    "        \n",
    "        print(f\"\\nColeta Inicial MongoDB (C4 - sessão) finalizada. {total_api_lidos_nesta_sessao_c4} processos lidos da API.\")\n",
    "        \n",
    "        final_search_after_c4 = locals().get('search_after_coleta_inicial_c4')\n",
    "        if final_search_after_c4 is not None:\n",
    "            salvar_estado_coleta_inicial_mongo(TRIBUNAL_ALVO_MONGO_COLETA, final_search_after_c4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3297fc7-305a-44f2-b704-3d0836420bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 5: Atualização Incremental de Processos e Movimentações (usando @timestamp da API e lendo do MongoDB)\n",
    "\n",
    "# Funções utilitárias e variáveis globais (mongo_client, colecoes, etc.) devem estar definidas.\n",
    "\n",
    "if 'mongo_client' not in locals() or mongo_client is None:\n",
    "    print(\"!!! ATENÇÃO: MongoDB não está conectado. Pulando Célula 5. !!!\")\n",
    "else:\n",
    "    print(f\"\\n--- Iniciando Atualização Incremental (API @timestamp) para: {TRIBUNAL_ALVO_MONGO_COLETA.upper()} (Célula 5) ---\")\n",
    "    \n",
    "    ultimo_ts_api_processado_mongo_obj_c5 = None\n",
    "    ultimo_ts_api_processado_mongo_str_c5 = None # String ISO para a query da API\n",
    "\n",
    "    # 1. Buscar o @timestamp mais recente do MongoDB da coleção de processos\n",
    "    try:\n",
    "        # Certifique-se que o campo é \"@timestamp\" e que está indexado (Célula 3)\n",
    "        documento_mais_recente_mongo = colecao_processos_mongo.find_one(\n",
    "            filter={}, # Sem filtro específico, apenas ordenação\n",
    "            sort=[(\"@timestamp\", pymongo.DESCENDING)] \n",
    "        )\n",
    "        \n",
    "        if documento_mais_recente_mongo and \"@timestamp\" in documento_mais_recente_mongo and \\\n",
    "           isinstance(documento_mais_recente_mongo[\"@timestamp\"], datetime):\n",
    "            ultimo_ts_api_processado_mongo_obj_c5 = documento_mais_recente_mongo[\"@timestamp\"]\n",
    "            # Formatar para string ISO com fuso UTC e milissegundos para a query da API\n",
    "            ultimo_ts_api_processado_mongo_str_c5 = ultimo_ts_api_processado_mongo_obj_c5.astimezone(timezone.utc).isoformat(timespec='milliseconds')\n",
    "            print(f\"  Último @timestamp da API encontrado no MongoDB: {ultimo_ts_api_processado_mongo_str_c5}\")\n",
    "        else:\n",
    "            print(\"  Nenhum @timestamp válido encontrado no MongoDB para este tribunal. \"\n",
    "                  \"Para atualização incremental, é esperado que a coleta inicial (Célula 4) já tenha rodado.\")\n",
    "            # Se for a primeira vez absoluta, talvez você queira um timestamp muito antigo.\n",
    "            # Mas para uma atualização, é melhor parar se não houver base.\n",
    "            # Ou usar um timestamp de \"início dos tempos\" se a coleção estiver vazia.\n",
    "            if colecao_processos_mongo.count_documents({}) == 0:\n",
    "                print(\"  Coleção de processos está vazia. Usando timestamp de início dos tempos para buscar tudo.\")\n",
    "                ultimo_ts_api_processado_mongo_obj_c5 = datetime(1970, 1, 1, tzinfo=timezone.utc)\n",
    "                ultimo_ts_api_processado_mongo_str_c5 = ultimo_ts_api_processado_mongo_obj_c5.isoformat(timespec='milliseconds')\n",
    "            else:\n",
    "                 print(\"  A coleção de processos não está vazia, mas não foi possível determinar o último @timestamp.\")\n",
    "                 print(\"  Verifique se os documentos têm o campo '@timestamp' como objeto datetime.\")\n",
    "                 # Não prosseguir se não puder determinar o ponto de partida de forma segura\n",
    "                 raise SystemExit(\"Não foi possível determinar o timestamp de partida para atualização.\")\n",
    "\n",
    "\n",
    "    except Exception as e_mongo_ts_c5:\n",
    "        print(f\"  ERRO ao buscar último @timestamp do MongoDB: {e_mongo_ts_c5}\")\n",
    "        raise SystemExit(\"Falha ao determinar o timestamp de partida para atualização.\")\n",
    "\n",
    "\n",
    "    print(f\"  Buscando processos/atualizações na API com @timestamp > {ultimo_ts_api_processado_mongo_str_c5}\")\n",
    "\n",
    "    search_after_atualizacao_c5 = None \n",
    "    processos_api_lidos_nesta_sessao_c5 = 0\n",
    "    \n",
    "    # max_timestamp_api_visto_nesta_sessao_obj_c5 rastreia o mais recente DESTA atualização.\n",
    "    # Inicia com o último conhecido do banco para garantir que só pegamos mais novos.\n",
    "    max_timestamp_api_visto_nesta_sessao_obj_c5 = ultimo_ts_api_processado_mongo_obj_c5\n",
    "\n",
    "    processos_batch_mongo_upsert_c5 = []\n",
    "    movimentacoes_para_processar_c5 = {} # {processo_id: [lista de novas movs dict]}\n",
    "    \n",
    "    # BATCH_SIZE_MONGO_INSERT_C5 (pode ser menor que o da coleta inicial)\n",
    "    BATCH_SIZE_MONGO_INSERT_C5 = 1000 # Ajuste conforme necessário\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            query_api_atualizacao = {\n",
    "                \"size\": TAMANHO_PAGINA_API,\n",
    "                \"query\": {\n",
    "                    \"range\": {\n",
    "                        \"@timestamp\": { \n",
    "                            \"gt\": ultimo_ts_api_processado_mongo_str_c5 \n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"sort\": [{\"@timestamp\": \"asc\"}]\n",
    "            }\n",
    "            if search_after_atualizacao_c5:\n",
    "                query_api_atualizacao[\"search_after\"] = search_after_atualizacao_c5\n",
    "            \n",
    "            response_data_att = consultar_api_datajud_mongo(TRIBUNAL_ALVO_MONGO_COLETA, query_api_atualizacao)\n",
    "\n",
    "            if response_data_att is None: break\n",
    "            hits_api_att = response_data_att.get('hits', {}).get('hits', [])\n",
    "            if not hits_api_att:\n",
    "                print(f\"  Nenhum novo processo ou atualização encontrado na API desde {ultimo_ts_api_processado_mongo_str_c5}.\")\n",
    "                break\n",
    "\n",
    "            # Processar os hits (lógica de transformação e append aos batches como na Célula 4)\n",
    "            for hit_api in hits_api_att:\n",
    "                source_data_api = hit_api.get(\"_source\", {})\n",
    "                id_cnj_api = hit_api.get(\"_id\") \n",
    "                if not id_cnj_api: continue\n",
    "                id_processo_mongo = id_cnj_api \n",
    "                \n",
    "                api_timestamp_str_doc = source_data_api.get(\"@timestamp\")\n",
    "                if api_timestamp_str_doc:\n",
    "                    current_api_dt_doc = string_para_datetime_utc(api_timestamp_str_doc)\n",
    "                    if current_api_dt_doc and \\\n",
    "                       (max_timestamp_api_visto_nesta_sessao_obj_c5 is None or \\\n",
    "                        current_api_dt_doc > max_timestamp_api_visto_nesta_sessao_obj_c5):\n",
    "                        max_timestamp_api_visto_nesta_sessao_obj_c5 = current_api_dt_doc\n",
    "\n",
    "                processo_doc_upsert = {\"_id\": id_processo_mongo}\n",
    "                processo_doc_upsert.update(source_data_api)\n",
    "                movs_api_para_processo = processo_doc_upsert.pop('movimentos', [])\n",
    "                processo_doc_upsert = converter_datas_no_documento(processo_doc_upsert)\n",
    "                processo_doc_upsert[\"_timestampColetaLocal\"] = datetime.now(timezone.utc)\n",
    "                processos_batch_mongo_upsert_c5.append(pymongo.ReplaceOne(\n",
    "                    {\"_id\": id_processo_mongo}, processo_doc_upsert, upsert=True\n",
    "                ))\n",
    "\n",
    "                novas_movs_formatadas = []\n",
    "                if movs_api_para_processo and isinstance(movs_api_para_processo, list):\n",
    "                    for mov_idx, mov_api_data in enumerate(movs_api_para_processo):\n",
    "                        if isinstance(mov_api_data, dict):\n",
    "                            mov_doc = mov_api_data.copy()\n",
    "                            mov_doc[\"processo_id\"] = id_processo_mongo\n",
    "                            mov_hash_parts = [\n",
    "                                str(id_processo_mongo), str(mov_api_data.get(\"codigo\")),\n",
    "                                str(mov_api_data.get(\"dataHora\")), \n",
    "                                json.dumps(mov_api_data.get(\"complementosTabelados\"), sort_keys=True),\n",
    "                                str(mov_idx) \n",
    "                            ]\n",
    "                            mov_doc[\"_id\"] = hashlib.sha1(\"\".join(filter(None, mov_hash_parts)).encode('utf-8')).hexdigest()\n",
    "                            mov_doc = converter_datas_no_documento(mov_doc)\n",
    "                            mov_doc[\"_timestampColetaLocal\"] = datetime.now(timezone.utc)\n",
    "                            novas_movs_formatadas.append(mov_doc)\n",
    "                movimentacoes_para_processar_c5[id_processo_mongo] = novas_movs_formatadas\n",
    "            \n",
    "            processos_api_lidos_nesta_sessao_c5 += len(hits_api_att)\n",
    "            print(f\"    Lidos {len(hits_api_att)} processos/atualizações da API. Total sessão: {processos_api_lidos_nesta_sessao_c5}\")\n",
    "\n",
    "            if len(processos_batch_mongo_upsert_c5) >= BATCH_SIZE_MONGO_INSERT_C5:\n",
    "                if processos_batch_mongo_upsert_c5:\n",
    "                    print(f\"    Aplicando {len(processos_batch_mongo_upsert_c5)} upserts de processos...\")\n",
    "                    try: colecao_processos_mongo.bulk_write(processos_batch_mongo_upsert_c5, ordered=False)\n",
    "                    except pymongo.errors.BulkWriteError as bwe_p: print(f\"      Erro BW (proc): {bwe_p.details}\")\n",
    "                    processos_batch_mongo_upsert_c5 = []\n",
    "                \n",
    "                if movimentacoes_para_processar_c5:\n",
    "                    print(f\"    Atualizando movimentações para {len(movimentacoes_para_processar_c5)} processos...\")\n",
    "                    ops_mov_batch_c5 = []\n",
    "                    for proc_id, movs_novas in movimentacoes_para_processar_c5.items():\n",
    "                        ops_mov_batch_c5.append(pymongo.DeleteMany({\"processo_id\": proc_id}))\n",
    "                        for mov_n in movs_novas:\n",
    "                            ops_mov_batch_c5.append(pymongo.ReplaceOne({\"_id\": mov_n[\"_id\"]}, mov_n, upsert=True))\n",
    "                    if ops_mov_batch_c5:\n",
    "                        try: colecao_movimentacoes_mongo.bulk_write(ops_mov_batch_c5, ordered=False)\n",
    "                        except pymongo.errors.BulkWriteError as bwe_m: print(f\"      Erro BW (mov): {bwe_m.details}\")\n",
    "                    movimentacoes_para_processar_c5 = {}\n",
    "                print(f\"    Lotes de atualização MongoDB processados.\")\n",
    "\n",
    "            search_after_atualizacao_c5 = hits_api_att[-1]['sort']\n",
    "            # O estado da coleta inicial (search_after) não é salvo aqui.\n",
    "            # Apenas o progresso desta atualização seria salvo se fosse um processo muito longo.\n",
    "            # Como estamos buscando \"novos desde o último X\", o próximo run da Célula 5\n",
    "            # vai buscar novamente o último @timestamp do Mongo.\n",
    "            time.sleep(0.1)\n",
    "\n",
    "    except Exception as e_fatal_att_c5:\n",
    "        print(f\"Erro fatal na atualização incremental (C5): {type(e_fatal_att_c5).__name__} - {e_fatal_att_c5}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        # Processar batches restantes\n",
    "        if processos_batch_mongo_upsert_c5:\n",
    "            print(f\"    Aplicando {len(processos_batch_mongo_upsert_c5)} upserts finais de processos...\")\n",
    "            # ... (lógica de bulk_write como acima)\n",
    "            try: colecao_processos_mongo.bulk_write(processos_batch_mongo_upsert_c5, ordered=False)\n",
    "            except pymongo.errors.BulkWriteError as bwe_p_f: print(f\"      Erro BW (final proc): {bwe_p_f.details}\")\n",
    "\n",
    "        if movimentacoes_para_processar_c5:\n",
    "            print(f\"    Atualizando movimentações finais para {len(movimentacoes_para_processar_c5)} processos...\")\n",
    "            # ... (lógica de bulk_write como acima)\n",
    "            ops_mov_batch_f_c5 = []\n",
    "            for proc_id_f, movs_f_novas in movimentacoes_para_processar_c5.items():\n",
    "                ops_mov_batch_f_c5.append(pymongo.DeleteMany({\"processo_id\": proc_id_f}))\n",
    "                for mov_f_n in movs_f_novas:\n",
    "                    ops_mov_batch_f_c5.append(pymongo.ReplaceOne({\"_id\": mov_f_n[\"_id\"]}, mov_f_n, upsert=True))\n",
    "            if ops_mov_batch_f_c5:\n",
    "                try: colecao_movimentacoes_mongo.bulk_write(ops_mov_batch_f_c5, ordered=False)\n",
    "                except pymongo.errors.BulkWriteError as bwe_m_f: print(f\"      Erro BW (final mov): {bwe_m_f.details}\")\n",
    "            \n",
    "        print(f\"\\nAtualização Incremental (C5 - sessão) finalizada. {processos_api_lidos_nesta_sessao_c5} processos/atualizações lidos da API.\")\n",
    "        \n",
    "        # O estado da coleta inicial (search_after_coleta_inicial) não é modificado por esta célula.\n",
    "        # O \"estado\" para a próxima execução da Célula 5 será lido diretamente do MongoDB.\n",
    "        # Se você quisesse salvar o último @timestamp processado por esta célula em um arquivo,\n",
    "        # poderia fazê-lo aqui, mas a ideia é depender do MongoDB.\n",
    "        if max_timestamp_api_visto_nesta_sessao_obj_c5:\n",
    "            print(f\"  O @timestamp mais recente da API processado nesta sessão foi: {max_timestamp_api_visto_nesta_sessao_obj_c5.isoformat()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b404c5d5-a221-42fb-ab4b-54a8e3b83fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 6: Rotina para Atualizar Movimentações de Processos EXISTENTES no MongoDB\n",
    "\n",
    "if mongo_client is None:\n",
    "    print(\"MongoDB não está conectado. Pulando Célula 6.\")\n",
    "else:\n",
    "    print(f\"\\n--- Iniciando Atualização de MOVIMENTAÇÕES em Processos Existentes no MongoDB para: {TRIBUNAL_ALVO_MONGO_COLETA.upper()} ---\")\n",
    "\n",
    "    # Data da última vez que esta rotina de atualização de movimentações rodou com sucesso\n",
    "    # Isso deve ser armazenado de forma persistente (ex: em outra coleção de metadados ou arquivo)\n",
    "    # Por agora, vamos simular ou buscar de um arquivo de estado específico para esta tarefa.\n",
    "    path_estado_att_mov_mongo = os.path.join(ESTADO_BASE_PATH, f\"att_mov_mongo_{TRIBUNAL_ALVO_MONGO_COLETA}_estado.json\")\n",
    "    \n",
    "    data_ultima_varredura_mov_str = None\n",
    "    if os.path.exists(path_estado_att_mov_mongo):\n",
    "        with open(path_estado_att_mov_mongo, 'r') as f_att_mov:\n",
    "            data_ultima_varredura_mov_str = json.load(f_att_mov).get(\"data_ultima_varredura_mov\")\n",
    "            if data_ultima_varredura_mov_str:\n",
    "                 print(f\"  Última varredura de movimentações em: {data_ultima_varredura_mov_str}\")\n",
    "\n",
    "    if not data_ultima_varredura_mov_str:\n",
    "        print(\"  Nenhuma data de última varredura de movimentações encontrada. Verificando processos com base em um período recente ou todos.\")\n",
    "        # Para a primeira execução, você pode querer um critério diferente ou processar um subconjunto.\n",
    "        # Aqui, vamos pegar processos atualizados na API desde o início dos tempos se não houver data.\n",
    "        data_ultima_varredura_mov_str = datetime(1900,1,1,tzinfo=timezone.utc).isoformat()\n",
    "\n",
    "\n",
    "    # 1. Buscar IDs de processos na API que foram atualizados desde a última varredura\n",
    "    #    Usaremos o @timestamp do processo para isso.\n",
    "    print(f\"  Buscando processos na API atualizados desde: {data_ultima_varredura_mov_str}\")\n",
    "    ids_processos_para_rebuscar_api = []\n",
    "    search_after_att_mov = None\n",
    "    \n",
    "    while True:\n",
    "        query_api_att_mov = {\n",
    "            \"size\": TAMANHO_PAGINA_API, # Pode ser maior aqui, já que só pegamos _id\n",
    "            \"_source\": [\"_id\"], # Só precisamos do ID do processo da API nesta fase\n",
    "            \"query\": {\n",
    "                \"range\": {\n",
    "                    \"@timestamp\": { # @timestamp do documento do processo na API\n",
    "                        \"gt\": data_ultima_varredura_mov_str \n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"sort\": [{\"@timestamp\": \"asc\"}]\n",
    "        }\n",
    "        if search_after_att_mov:\n",
    "            query_api_att_mov[\"search_after\"] = search_after_att_mov\n",
    "        \n",
    "        response_att_mov = consultar_api_datajud_mongo(TRIBUNAL_ALVO_MONGO_COLETA, query_api_att_mov)\n",
    "        if response_att_mov is None or not response_att_mov.get('hits', {}).get('hits'):\n",
    "            break\n",
    "        \n",
    "        hits_att_mov = response_att_mov['hits']['hits']\n",
    "        for hit in hits_att_mov:\n",
    "            ids_processos_para_rebuscar_api.append(hit.get(\"_id\"))\n",
    "        \n",
    "        if len(hits_att_mov) < TAMANHO_PAGINA_API: break\n",
    "        search_after_att_mov = hits_att_mov[-1]['sort']\n",
    "        print(f\"    Coletados {len(ids_processos_para_rebuscar_api)} IDs de processos atualizados na API...\")\n",
    "\n",
    "    print(f\"  Total de {len(ids_processos_para_rebuscar_api)} processos identificados na API para re-verificação de movimentações.\")\n",
    "\n",
    "    # 2. Para cada ID de processo identificado, re-buscar o documento completo da API\n",
    "    #    e atualizar no MongoDB (processo e suas movimentações)\n",
    "    BATCH_SIZE_REBUSCA_MONGO = 50 # Quantos processos completos re-buscar da API por vez\n",
    "    processos_atualizados_mov_sessao = 0\n",
    "    \n",
    "    for i in range(0, len(ids_processos_para_rebuscar_api), BATCH_SIZE_REBUSCA_MONGO):\n",
    "        lote_ids_rebusca = ids_processos_para_rebuscar_api[i : i + BATCH_SIZE_REBUSCA_MONGO]\n",
    "        print(f\"\\n  Rebuscando lote de {len(lote_ids_rebusca)} processos da API...\")\n",
    "        \n",
    "        query_lote_rebusca = {\n",
    "            \"query\": {\"ids\": {\"values\": lote_ids_rebusca}},\n",
    "            \"size\": len(lote_ids_rebusca)\n",
    "        }\n",
    "        response_lote_completo = consultar_api_datajud_mongo(TRIBUNAL_ALVO_MONGO_COLETA, query_lote_rebusca)\n",
    "\n",
    "        if response_lote_completo and response_lote_completo.get('hits', {}).get('hits'):\n",
    "            hits_lote_completo = response_lote_completo['hits']['hits']\n",
    "            \n",
    "            processos_para_upsert_mongo = []\n",
    "            movimentacoes_para_deletar_e_inserir_mongo = [] # Lista de (processo_id, lista_de_novas_movs)\n",
    "            ids_processos_neste_lote_api = []\n",
    "\n",
    "            for hit_completo in hits_lote_completo:\n",
    "                source_completo = hit_completo.get(\"_source\", {})\n",
    "                id_cnj_completo = hit_completo.get(\"_id\")\n",
    "                if not id_cnj_completo: continue\n",
    "\n",
    "                ids_processos_neste_lote_api.append(id_cnj_completo)\n",
    "                id_processo_mongo_att = id_cnj_completo\n",
    "                \n",
    "                processo_doc_att = {\"_id\": id_processo_mongo_att}\n",
    "                processo_doc_att.update(source_completo)\n",
    "                movs_api_att = processo_doc_att.pop('movimentos', [])\n",
    "                processo_doc_att = converter_datas_no_documento(processo_doc_att)\n",
    "                processo_doc_att[\"timestampColetaLocal\"] = datetime.now(timezone.utc) # Atualiza timestamp\n",
    "                processos_para_upsert_mongo.append(pymongo.ReplaceOne({\"_id\": id_processo_mongo_att}, processo_doc_att, upsert=True))\n",
    "\n",
    "                novas_movs_para_este_processo = []\n",
    "                if movs_api_att:\n",
    "                    for mov_idx_att, mov_api_att_data in enumerate(movs_api_att):\n",
    "                        if isinstance(mov_api_att_data, dict):\n",
    "                            mov_doc_att = mov_api_att_data.copy()\n",
    "                            mov_doc_att[\"processo_id\"] = id_processo_mongo_att\n",
    "                            mov_doc_att = converter_datas_no_documento(mov_doc_att)\n",
    "                            mov_doc_att[\"timestampColetaLocal\"] = datetime.now(timezone.utc)\n",
    "                            novas_movs_para_este_processo.append(mov_doc_att)\n",
    "                if novas_movs_para_este_processo: # Mesmo se vazio, precisamos deletar as antigas\n",
    "                     movimentacoes_para_deletar_e_inserir_mongo.append({\"processo_id\": id_processo_mongo_att, \"novas_movs\": novas_movs_para_este_processo})\n",
    "\n",
    "\n",
    "            # Executar operações no MongoDB\n",
    "            if processos_para_upsert_mongo:\n",
    "                print(f\"    Atualizando/Inserindo {len(processos_para_upsert_mongo)} processos no MongoDB...\")\n",
    "                colecao_processos_mongo.bulk_write(processos_para_upsert_mongo, ordered=False)\n",
    "            \n",
    "            if movimentacoes_para_deletar_e_inserir_mongo:\n",
    "                print(f\"    Atualizando movimentações para {len(movimentacoes_para_deletar_e_inserir_mongo)} processos...\")\n",
    "                for item_mov_att in movimentacoes_para_deletar_e_inserir_mongo:\n",
    "                    pid = item_mov_att[\"processo_id\"]\n",
    "                    novas_movs = item_mov_att[\"novas_movs\"]\n",
    "                    # Deletar todas as movimentações antigas deste processo\n",
    "                    colecao_movimentacoes_mongo.delete_many({\"processo_id\": pid})\n",
    "                    # Inserir as novas movimentações (se houver)\n",
    "                    if novas_movs:\n",
    "                        colecao_movimentacoes_mongo.insert_many(novas_movs, ordered=False)\n",
    "            \n",
    "            processos_atualizados_mov_sessao += len(ids_processos_neste_lote_api)\n",
    "            print(f\"    Lote de {len(ids_processos_neste_lote_api)} processos teve movimentações atualizadas. Total sessão: {processos_atualizados_mov_sessao}\")\n",
    "        time.sleep(0.5) # Pausa entre lotes de rebusca\n",
    "\n",
    "    # Salvar a data/hora atual como a última varredura\n",
    "    with open(path_estado_att_mov_mongo, 'w') as f_att_mov:\n",
    "        json.dump({\"data_ultima_varredura_mov\": datetime.now(timezone.utc).isoformat()}, f_att_mov)\n",
    "    print(f\"  Data da última varredura de movimentações atualizada.\")\n",
    "    print(f\"\\nAtualização de MOVIMENTAÇÕES (sessão) finalizada. {processos_atualizados_mov_sessao} processos verificados/atualizados.\")\n",
    "\n",
    "# --- Célula Final: Fechar Conexão MongoDB (se não for mais usar) ---\n",
    "# if mongo_client:\n",
    "#     mongo_client.close()\n",
    "#     print(\"\\nConexão com MongoDB fechada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c6336e-e647-491e-8885-9e25c38452ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula Nova: Amostragem Estratificada de Processos da API vs MongoDB (COM AJUSTE PARA DATAS MIN/MAX)\n",
    "\n",
    "import pandas as pd\n",
    "from dateutil.relativedelta import relativedelta \n",
    "\n",
    "# Funções e variáveis globais (consultar_api_datajud_mongo, TRIBUNAL_ALVO_MONGO_COLETA, \n",
    "# HEADERS_API, mongo_client, colecao_processos_mongo, string_para_datetime_utc) devem estar definidas.\n",
    "\n",
    "print(f\"\\n--- Iniciando Amostragem Estratificada para {TRIBUNAL_ALVO_MONGO_COLETA.upper()} (com ajuste de data min) ---\")\n",
    "\n",
    "if 'mongo_client' not in locals() or mongo_client is None:\n",
    "    print(\"!!! ATENÇÃO: MongoDB não está conectado. Não é possível verificar o banco de dados. !!!\")\n",
    "    db_conectado_amostragem = False\n",
    "else:\n",
    "    db_conectado_amostragem = True\n",
    "\n",
    "NUMERO_TOTAL_AMOSTRAS = 27000\n",
    "# NUMERO_TOTAL_AMOSTRAS = 200 # Para teste rápido\n",
    "NUMERO_DE_INTERVALOS_TEMPO = 100 \n",
    "AMOSTRAS_POR_INTERVALO = NUMERO_TOTAL_AMOSTRAS // NUMERO_DE_INTERVALOS_TEMPO\n",
    "if AMOSTRAS_POR_INTERVALO == 0: AMOSTRAS_POR_INTERVALO = 1 \n",
    "\n",
    "processos_amostrados_api = []\n",
    "processos_nao_encontrados_mongo_amostragem = []\n",
    "total_consultados_api_amostragem = 0\n",
    "\n",
    "try:\n",
    "    # 1. Obter data de ajuizamento mínima (a partir do 100º registro) e máxima da API\n",
    "    print(\"  Buscando data de ajuizamento mínima (a partir do 100º) e máxima da API...\")\n",
    "    \n",
    "    # Para data mínima, pegar o 100º processo mais antigo\n",
    "    # Fazemos uma query pedindo 1 processo, pulando os primeiros 99 (from: 99)\n",
    "    query_min_data_offset = {\n",
    "        \"size\": 1, \n",
    "        \"from\": 99, # Pula os primeiros 99 para pegar o 100º\n",
    "        \"sort\": [{\"dataAjuizamento\": \"asc\"}], \n",
    "        \"_source\": [\"dataAjuizamento\", \"numeroProcesso\", \"_id\"] # Pegar ID para log se necessário\n",
    "    }\n",
    "    \n",
    "    query_max_data = {\n",
    "        \"size\": 1, \n",
    "        \"sort\": [{\"dataAjuizamento\": \"desc\"}], \n",
    "        \"_source\": [\"dataAjuizamento\"]\n",
    "    }\n",
    "\n",
    "    resp_min_data_offset = consultar_api_datajud_mongo(TRIBUNAL_ALVO_MONGO_COLETA, query_min_data_offset)\n",
    "    resp_max_data = consultar_api_datajud_mongo(TRIBUNAL_ALVO_MONGO_COLETA, query_max_data)\n",
    "\n",
    "    data_ajuizamento_min_str = None\n",
    "    data_ajuizamento_max_str = None\n",
    "\n",
    "    if resp_min_data_offset and resp_min_data_offset.get('hits', {}).get('hits'):\n",
    "        hit_min = resp_min_data_offset['hits']['hits'][0]\n",
    "        data_ajuizamento_min_str = hit_min['_source'].get('dataAjuizamento')\n",
    "        print(f\"    Data de ajuizamento do 100º processo mais antigo (usada como MÍNIMA): {data_ajuizamento_min_str}\")\n",
    "        print(f\"      (ID do processo usado para data mínima: {hit_min.get('_id')})\")\n",
    "    else:\n",
    "        print(\"    AVISO: Não foi possível obter o 100º processo mais antigo. Tentando o 1º.\")\n",
    "        # Fallback para o primeiro se não conseguir o 100º (ex: menos de 100 processos no total)\n",
    "        query_min_data_primeiro = {\"size\": 1, \"sort\": [{\"dataAjuizamento\": \"asc\"}], \"_source\": [\"dataAjuizamento\"]}\n",
    "        resp_min_data_primeiro = consultar_api_datajud_mongo(TRIBUNAL_ALVO_MONGO_COLETA, query_min_data_primeiro)\n",
    "        if resp_min_data_primeiro and resp_min_data_primeiro.get('hits', {}).get('hits'):\n",
    "            data_ajuizamento_min_str = resp_min_data_primeiro['hits']['hits'][0]['_source'].get('dataAjuizamento')\n",
    "            print(f\"    Data de ajuizamento do 1º processo mais antigo (usada como MÍNIMA): {data_ajuizamento_min_str}\")\n",
    "\n",
    "\n",
    "    if resp_max_data and resp_max_data.get('hits', {}).get('hits'):\n",
    "        data_ajuizamento_max_str = resp_max_data['hits']['hits'][0]['_source'].get('dataAjuizamento')\n",
    "        print(f\"    Data de ajuizamento MÁXIMA: {data_ajuizamento_max_str}\")\n",
    "\n",
    "\n",
    "    if not data_ajuizamento_min_str or not data_ajuizamento_max_str:\n",
    "        print(\"  ERRO: Não foi possível obter o intervalo de datas de ajuizamento da API.\")\n",
    "        raise SystemExit(\"Falha ao obter datas min/max.\")\n",
    "\n",
    "    data_min = string_para_datetime_utc(data_ajuizamento_min_str)\n",
    "    data_max = string_para_datetime_utc(data_ajuizamento_max_str)\n",
    "    \n",
    "    if not data_min or not data_max or data_min > data_max: # Adicionada verificação de data_min > data_max\n",
    "        print(\"  ERRO: Falha ao converter as strings de data min/max ou data mínima é maior que a máxima.\")\n",
    "        print(f\"    Data min string: {data_ajuizamento_min_str} -> Convertida: {data_min}\")\n",
    "        print(f\"    Data max string: {data_ajuizamento_max_str} -> Convertida: {data_max}\")\n",
    "        raise SystemExit(\"Falha na conversão ou lógica de datas.\")\n",
    "\n",
    "    print(f\"  Intervalo de data de ajuizamento (ajustado) na API: {data_min.date()} a {data_max.date()}\")\n",
    "\n",
    "    # 2. Criar os intervalos de tempo (lógica mantida)\n",
    "    delta_total_dias = (data_max - data_min).days\n",
    "    if delta_total_dias <= 0 : delta_total_dias = 1\n",
    "    \n",
    "    tamanho_intervalo_dias = max(1, delta_total_dias // NUMERO_DE_INTERVALOS_TEMPO)\n",
    "    \n",
    "    print(f\"  Delta total de dias (ajustado): {delta_total_dias}. Cada um dos {NUMERO_DE_INTERVALOS_TEMPO} intervalos terá aprox. {tamanho_intervalo_dias} dias.\")\n",
    "    print(f\"  Tentando coletar {AMOSTRAS_POR_INTERVALO} amostras por intervalo.\")\n",
    "\n",
    "    data_inicio_intervalo = data_min\n",
    "    \n",
    "    for i in range(NUMERO_DE_INTERVALOS_TEMPO):\n",
    "        if total_consultados_api_amostragem >= NUMERO_TOTAL_AMOSTRAS:\n",
    "            print(f\"  Limite de {NUMERO_TOTAL_AMOSTRAS} amostras atingido. Parando.\")\n",
    "            break\n",
    "\n",
    "        data_fim_intervalo = data_inicio_intervalo + pd.Timedelta(days=tamanho_intervalo_dias)\n",
    "        if data_fim_intervalo > data_max + pd.Timedelta(days=1):\n",
    "            data_fim_intervalo = data_max + pd.Timedelta(days=1)\n",
    "        \n",
    "        if data_inicio_intervalo >= data_fim_intervalo:\n",
    "            if data_inicio_intervalo <= data_max:\n",
    "                 data_fim_intervalo = data_max + pd.Timedelta(days=1)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        print(f\"\\n  Amostrando Intervalo {i+1}/{NUMERO_DE_INTERVALOS_TEMPO}: \"\n",
    "              f\"{data_inicio_intervalo.strftime('%Y-%m-%dT%H:%M:%S.%fZ')} a \"\n",
    "              f\"{data_fim_intervalo.strftime('%Y-%m-%dT%H:%M:%S.%fZ')}\")\n",
    "\n",
    "        query_intervalo = {\n",
    "            \"size\": AMOSTRAS_POR_INTERVALO,\n",
    "            \"query\": {\n",
    "                \"range\": {\n",
    "                    \"dataAjuizamento\": {\n",
    "                        \"gte\": data_inicio_intervalo.isoformat().replace('+00:00', 'Z'),\n",
    "                        \"lt\": data_fim_intervalo.isoformat().replace('+00:00', 'Z')\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"sort\": [{\"@timestamp\": \"asc\"}], \n",
    "            \"_source\": [\"numeroProcesso\", \"@timestamp\", \"_id\"] # Adicionado _id para consistência\n",
    "        }\n",
    "        \n",
    "        response_intervalo = consultar_api_datajud_mongo(TRIBUNAL_ALVO_MONGO_COLETA, query_intervalo)\n",
    "        \n",
    "        if response_intervalo and response_intervalo.get('hits', {}).get('hits'):\n",
    "            hits_intervalo = response_intervalo['hits']['hits']\n",
    "            print(f\"    API retornou {len(hits_intervalo)} processos para este intervalo.\")\n",
    "            \n",
    "            for hit in hits_intervalo:\n",
    "                if total_consultados_api_amostragem >= NUMERO_TOTAL_AMOSTRAS: break\n",
    "                \n",
    "                source = hit.get(\"_source\", {})\n",
    "                id_api_amostra = hit.get(\"_id\") \n",
    "                num_proc_api_amostra = source.get(\"numeroProcesso\")\n",
    "                ts_api_amostra = source.get(\"@timestamp\")\n",
    "                \n",
    "                id_mongo_check_amostra = id_api_amostra\n",
    "\n",
    "                if id_mongo_check_amostra and ts_api_amostra:\n",
    "                    processos_amostrados_api.append({\n",
    "                        \"id_mongo\": id_mongo_check_amostra,\n",
    "                        \"numeroProcesso\": num_proc_api_amostra,\n",
    "                        \"@timestamp_api\": ts_api_amostra\n",
    "                    })\n",
    "                    total_consultados_api_amostragem += 1\n",
    "        else:\n",
    "            print(f\"    Nenhum processo encontrado pela API para este intervalo.\")\n",
    "\n",
    "        data_inicio_intervalo = data_fim_intervalo \n",
    "        if data_inicio_intervalo > data_max:\n",
    "            break \n",
    "        \n",
    "        time.sleep(0.2)\n",
    "\n",
    "    # 3. Verificar as amostras no MongoDB (lógica mantida)\n",
    "    if processos_amostrados_api and db_conectado_amostragem:\n",
    "        print(f\"\\n--- Verificando {len(processos_amostrados_api)} Processos Amostrados no MongoDB ---\")\n",
    "        encontrados_mongo_amostragem = 0\n",
    "        \n",
    "        for proc_amostra in processos_amostrados_api:\n",
    "            doc_mongo = colecao_processos_mongo.find_one({\"_id\": proc_amostra[\"id_mongo\"]})\n",
    "            if doc_mongo:\n",
    "                encontrados_mongo_amostragem += 1\n",
    "            else:\n",
    "                processos_nao_encontrados_mongo_amostragem.append(proc_amostra)\n",
    "        \n",
    "        print(f\"\\n  Resumo da Amostragem:\")\n",
    "        print(f\"    - Total de amostras da API coletadas: {len(processos_amostrados_api)}\")\n",
    "        print(f\"    - Amostras encontradas no MongoDB: {encontrados_mongo_amostragem}\")\n",
    "        print(f\"    - Amostras NÃO encontradas no MongoDB: {len(processos_nao_encontrados_mongo_amostragem)}\")\n",
    "\n",
    "        if processos_nao_encontrados_mongo_amostragem:\n",
    "            print(\"\\n    Detalhe das amostras NÃO encontradas no MongoDB (primeiros 20):\")\n",
    "            for i_nao_enc, p_nao_enc in enumerate(processos_nao_encontrados_mongo_amostragem):\n",
    "                if i_nao_enc >= 20:\n",
    "                    print(f\"      ... e mais {len(processos_nao_encontrados_mongo_amostragem) - 20} não listados aqui.\")\n",
    "                    break\n",
    "                print(f\"      - ID/Num: {p_nao_enc['id_mongo']} (@ts API: {p_nao_enc['@timestamp_api']})\")\n",
    "            \n",
    "            if len(processos_nao_encontrados_mongo_amostragem) > 0:\n",
    "                print(\"\\n    CONCLUSÃO DA AMOSTRAGEM: Foram encontradas lacunas na sua base de dados local.\")\n",
    "            # (Resto da lógica de conclusão como antes)\n",
    "\n",
    "        elif len(processos_amostrados_api) > 0 :\n",
    "             print(\"\\n    CONCLUSÃO DA AMOSTRAGEM: Todas as amostras coletadas da API foram encontradas no MongoDB.\")\n",
    "             print(\"    Isso é um bom indicativo, mas não uma garantia absoluta de 100% de integridade.\")\n",
    "\n",
    "    elif not db_conectado_amostragem:\n",
    "        print(\"\\n  Verificação no MongoDB não realizada (cliente não conectado).\")\n",
    "    elif not processos_amostrados_api:\n",
    "        print(\"\\n  Nenhuma amostra foi coletada da API para verificação.\")\n",
    "\n",
    "except SystemExit as se:\n",
    "    print(f\"Script interrompido: {se}\")\n",
    "except Exception as e_amostragem:\n",
    "    print(f\"ERRO GERAL durante a amostragem: {type(e_amostragem).__name__} - {e_amostragem}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd502d5-33eb-40c4-a91a-c44da0b9cf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula Nova: Verificar Últimos Processos da API vs MongoDB\n",
    "\n",
    "# As funções utilitárias (consultar_api_datajud_mongo) e variáveis globais \n",
    "# (TRIBUNAL_ALVO_MONGO_COLETA, HEADERS_API, mongo_client, colecao_processos_mongo)\n",
    "# devem estar definidas e o mongo_client conectado.\n",
    "\n",
    "print(f\"\\n--- Verificando os 20 Processos Mais Recentes da API para {TRIBUNAL_ALVO_MONGO_COLETA.upper()} vs MongoDB ---\")\n",
    "\n",
    "if 'mongo_client' not in locals() or mongo_client is None:\n",
    "    print(\"!!! ATENÇÃO: MongoDB não está conectado. Não é possível verificar o banco de dados. !!!\")\n",
    "    # Você pode querer parar aqui ou apenas pular a verificação do MongoDB\n",
    "    # raise SystemExit(\"MongoDB não conectado.\")\n",
    "    db_conectado = False\n",
    "else:\n",
    "    db_conectado = True\n",
    "\n",
    "# 1. Consultar a API do CNJ pelos processos mais recentes\n",
    "query_api_recentes = {\n",
    "    \"size\": 20,\n",
    "    \"query\": {\"match_all\": {}}, # Pegar todos, mas a ordenação trará os mais recentes\n",
    "    \"sort\": [{\"@timestamp\": \"desc\"}], # Ordenar por @timestamp descendente para pegar os mais novos\n",
    "    \"_source\": [\"numeroProcesso\", \"@timestamp\"] # Pedir apenas os campos que precisamos da API\n",
    "                                            # Se você usa o hit['_id'] como ID no Mongo, peça ele também:\n",
    "                                            # \"_source\": [\"_id\", \"numeroProcesso\", \"@timestamp\"] \n",
    "}\n",
    "\n",
    "print(f\"  Consultando API para obter os 20 processos mais recentes do {TRIBUNAL_ALVO_MONGO_COLETA.upper()}...\")\n",
    "response_data_recentes = consultar_api_datajud_mongo(TRIBUNAL_ALVO_MONGO_COLETA, query_api_recentes)\n",
    "\n",
    "processos_recentes_api = []\n",
    "if response_data_recentes and response_data_recentes.get('hits', {}).get('hits'):\n",
    "    hits_recentes = response_data_recentes['hits']['hits']\n",
    "    print(f\"  API retornou {len(hits_recentes)} processos.\")\n",
    "    for hit in hits_recentes:\n",
    "        source = hit.get(\"_source\", {})\n",
    "        id_api = hit.get(\"_id\") # O ID completo da API, ex: TJPB_CLASSE_GRAU_ORGAO_NUMERO\n",
    "        num_processo_api = source.get(\"numeroProcesso\") # O número do processo CNJ formatado\n",
    "        timestamp_api_str = source.get(\"@timestamp\")\n",
    "        \n",
    "        # Decida qual ID você usa como chave primária (_id) no seu MongoDB\n",
    "        # Se for o id_api (TJPB_CLASSE_...), use ele.\n",
    "        # Se for o num_processo_api, use ele.\n",
    "        # No seu código da Célula 4, você usou id_cnj_original_api (que é hit.get(\"_id\")) como _id do Mongo.\n",
    "        id_para_verificar_no_mongo = id_api \n",
    "        \n",
    "        if id_para_verificar_no_mongo and timestamp_api_str:\n",
    "            processos_recentes_api.append({\n",
    "                \"id_api_ou_numero\": id_para_verificar_no_mongo,\n",
    "                \"numeroProcesso\": num_processo_api, # Para referência\n",
    "                \"@timestamp_api\": timestamp_api_str\n",
    "            })\n",
    "else:\n",
    "    print(\"  Não foi possível obter os processos recentes da API ou nenhum foi retornado.\")\n",
    "\n",
    "# 2. Verificar se esses processos estão no MongoDB\n",
    "if processos_recentes_api and db_conectado:\n",
    "    print(f\"\\n  Verificando {len(processos_recentes_api)} processos recentes no MongoDB (Coleção: {COLECAO_PROCESSOS_MONGO_NOME}):\")\n",
    "    \n",
    "    encontrados_no_mongo = 0\n",
    "    nao_encontrados_no_mongo = []\n",
    "\n",
    "    for proc_api in processos_recentes_api:\n",
    "        id_check = proc_api[\"id_api_ou_numero\"]\n",
    "        # A query no MongoDB usa o campo que você definiu como _id na sua coleção de processos\n",
    "        documento_mongo = colecao_processos_mongo.find_one({\"_id\": id_check}) \n",
    "        \n",
    "        if documento_mongo:\n",
    "            encontrados_no_mongo += 1\n",
    "            print(f\"    ENCONTRADO: Processo ID '{id_check}' (API @ts: {proc_api['@timestamp_api']}) existe no MongoDB.\")\n",
    "            # Você pode querer comparar o @timestamp da API com algum timestamp do seu documento Mongo\n",
    "            # Por exemplo, se você armazena o @timestamp da API no seu documento Mongo:\n",
    "            # ts_mongo_api = documento_mongo.get(\"@timestamp\") # Supondo que você salvou o @timestamp original\n",
    "            # if ts_mongo_api and ts_mongo_api != proc_api['@timestamp_api']:\n",
    "            #     print(f\"      AVISO: @timestamp diferente! API: {proc_api['@timestamp_api']}, MongoDB: {ts_mongo_api}\")\n",
    "        else:\n",
    "            nao_encontrados_no_mongo.append(proc_api)\n",
    "            print(f\"    NÃO ENCONTRADO: Processo ID '{id_check}' (API @ts: {proc_api['@timestamp_api']}) NÃO existe no MongoDB.\")\n",
    "\n",
    "    print(f\"\\n  Resumo da Verificação:\")\n",
    "    print(f\"    - Processos recentes da API analisados: {len(processos_recentes_api)}\")\n",
    "    print(f\"    - Encontrados no MongoDB: {encontrados_no_mongo}\")\n",
    "    print(f\"    - Não encontrados no MongoDB: {len(nao_encontrados_no_mongo)}\")\n",
    "    \n",
    "    if nao_encontrados_no_mongo:\n",
    "        print(\"\\n    Detalhe dos processos NÃO encontrados no MongoDB:\")\n",
    "        for p_nao_enc in nao_encontrados_no_mongo:\n",
    "            print(f\"      - ID/Num: {p_nao_enc['id_api_ou_numero']}, API @timestamp: {p_nao_enc['@timestamp_api']}\")\n",
    "        print(\"\\n    Se houver processos não encontrados, sua Célula 5 (atualização incremental)\")\n",
    "        print(\"    deveria pegá-los na próxima execução, assumindo que o 'ultimo_ts_api_processado_global_str_c5'\")\n",
    "        print(\"    no seu arquivo de estado é anterior ao @timestamp desses processos.\")\n",
    "\n",
    "elif not db_conectado:\n",
    "    print(\"\\n  Verificação no MongoDB não realizada pois o cliente não está conectado.\")\n",
    "else:\n",
    "    print(\"\\n  Nenhum processo recente obtido da API para verificar no MongoDB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dde4a09-91c3-4f6d-851d-2c4afb1a85aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
